{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('fake-news-pair-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>It took 30 years of cooking oil to know that o...</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  tid1  tid2                          title1_zh  \\\n",
       "0   0     0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "1   3     2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   1     2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "3   2     2     5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "4   9     6     7               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "\n",
       "                    title2_zh  \\\n",
       "0    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "2        GDP首超香港？深圳澄清：还差一点点……   \n",
       "3  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "4     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n",
       "\n",
       "                                           title1_en  \\\n",
       "0  There are two new old-age insurance benefits f...   \n",
       "1  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "3  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "4  \"How to discriminate oil from gutter oil by me...   \n",
       "\n",
       "                                           title2_en      label  \n",
       "0  Police disprove \"bird's nest congress each per...  unrelated  \n",
       "1  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "2  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "3  Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "4  It took 30 years of cooking oil to know that o...     agreed  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_title = ((df_train['title1_zh'].isnull()) \\\n",
    "               | (df_train['title2_zh'].isnull()) \\\n",
    "               | (df_train['title2_zh'] == '') \\\n",
    "               | (df_train['title2_zh'] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320552\n",
      "320545\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "\n",
    "# Remove null rows\n",
    "df_train = df_train[~empty_title]\n",
    "\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             2017养老保险又新增两项，农村老人人人可申领，你领到了吗\n",
       "1         \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港\n",
       "2         \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港\n",
       "3         \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港\n",
       "4                      \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油\n",
       "                        ...                \n",
       "320547      萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大\n",
       "320548      萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大\n",
       "320549      萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大\n",
       "320550      萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大\n",
       "320551        萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗\n",
       "Name: title1_zh, Length: 320545, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head().loc[3,'title1_zh']\n",
    "df_train.title1_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265695\n"
     ]
    }
   ],
   "source": [
    "# 去掉长度大于30的句子\n",
    "MAX_LENGTH = 30\n",
    "df_train = df_train[~df_train.title1_zh.apply(lambda x: (len(x) > MAX_LENGTH))]\n",
    "df_train = df_train[~df_train.title2_zh.apply(lambda x: (len(x) > MAX_LENGTH))]\n",
    "\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只取一瓢\n",
    "SAMPLE_RATIO = 0.01\n",
    "\n",
    "df_train = df_train.sample(frac = SAMPLE_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2657\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置index，只保存需要的标签，最后重命名\n",
    "df_train.reset_index\n",
    "df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
    "df_train.columns = ['text_a', 'text_b', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102861</th>\n",
       "      <td>别说你都知道，华为手机音量键还有这样的操作！</td>\n",
       "      <td>华为手机音量键还隐藏了这么多功能，玩手机这么久才发现，快试试</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41004</th>\n",
       "      <td>一块陈皮能顶10副药，治疗膝盖疼效果惊人，家有膝盖疼的要存！</td>\n",
       "      <td>一颗洋葱顶10贴膏药，治疗膝盖疼效果惊人，家有膝盖疼的要存！</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11974</th>\n",
       "      <td>29岁释小龙回归，前女友嫌弃他，今事业辉煌前任离婚</td>\n",
       "      <td>能否与王菲谢霆锋一样何洁离婚后找释小龙</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>2017这几种袁大头市场才叫火爆，最贵的卖了598万我的天！</td>\n",
       "      <td>袁大头真品银元市场火爆 现在可以免费拍卖出手</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261192</th>\n",
       "      <td>王思聪放弃网红追腾讯千金，两人机场牵手相约吃饭？网友：我反对</td>\n",
       "      <td>王思聪预追马总千金？马化腾亲自辟谣！</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text_a                          text_b  \\\n",
       "102861          别说你都知道，华为手机音量键还有这样的操作！  华为手机音量键还隐藏了这么多功能，玩手机这么久才发现，快试试   \n",
       "41004   一块陈皮能顶10副药，治疗膝盖疼效果惊人，家有膝盖疼的要存！  一颗洋葱顶10贴膏药，治疗膝盖疼效果惊人，家有膝盖疼的要存！   \n",
       "11974        29岁释小龙回归，前女友嫌弃他，今事业辉煌前任离婚             能否与王菲谢霆锋一样何洁离婚后找释小龙   \n",
       "5215    2017这几种袁大头市场才叫火爆，最贵的卖了598万我的天！          袁大头真品银元市场火爆 现在可以免费拍卖出手   \n",
       "261192  王思聪放弃网红追腾讯千金，两人机场牵手相约吃饭？网友：我反对              王思聪预追马总千金？马化腾亲自辟谣！   \n",
       "\n",
       "            label  \n",
       "102861     agreed  \n",
       "41004   unrelated  \n",
       "11974   unrelated  \n",
       "5215    unrelated  \n",
       "261192  disagreed  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unrelated    0.684607\n",
       "agreed       0.283779\n",
       "disagreed    0.031615\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261192</th>\n",
       "      <td>王思聪放弃网红追腾讯千金，两人机场牵手相约吃饭？网友：我反对</td>\n",
       "      <td>王思聪预追马总千金？马化腾亲自辟谣！</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121153</th>\n",
       "      <td>吴亦凡被爆吸毒 13秒“摇头”影片疯传</td>\n",
       "      <td>吴亦凡吸毒视频疑造谣 工作室声明及时成公关范本</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58674</th>\n",
       "      <td>中超亚军主帅尚且下课，第六鲁能换帅亦成定局，新帅会是他吗？</td>\n",
       "      <td>鲁能换帅系谣言，那么换帅俱乐部会是哪几家呢？</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216510</th>\n",
       "      <td>最火的团队年终奖：王者荣耀团队年终奖：100个月的工资</td>\n",
       "      <td>王者荣耀：农药团队年终奖100个月工资，官方辟谣不属实</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232629</th>\n",
       "      <td>桃子和西瓜一起吃会致命？专家：我常一起吃</td>\n",
       "      <td>「读网」桃子西瓜一起吃不会丧命 不要相信谣言</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174764</th>\n",
       "      <td>张卫健吸毒被带走</td>\n",
       "      <td>张卫健辟谣什么 事件来龙去脉曝光一个举动被传吸毒</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113003</th>\n",
       "      <td>又一明星倒下？宋小宝被传患癌去世，不会吧</td>\n",
       "      <td>以讹传讹，宋小宝去世了！别逗了，宋小宝都出来辟谣了</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294025</th>\n",
       "      <td>网传“2017年免缴新农合医保费”，你当真了？</td>\n",
       "      <td>辟谣：2017年新农合不用缴费，已开始退款？假的！</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257810</th>\n",
       "      <td>独生子女将无法继承父母房产</td>\n",
       "      <td>独生子女将无法继承父母房产？谣言</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128097</th>\n",
       "      <td>喝酒没开车也算酒驾！又来一条新交规，昆明老司机们赶紧看</td>\n",
       "      <td>安徽车主速看！酒后躺车内算酒驾吗？交警辟谣了……</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text_a                       text_b      label\n",
       "261192  王思聪放弃网红追腾讯千金，两人机场牵手相约吃饭？网友：我反对           王思聪预追马总千金？马化腾亲自辟谣！  disagreed\n",
       "121153             吴亦凡被爆吸毒 13秒“摇头”影片疯传      吴亦凡吸毒视频疑造谣 工作室声明及时成公关范本  disagreed\n",
       "58674    中超亚军主帅尚且下课，第六鲁能换帅亦成定局，新帅会是他吗？       鲁能换帅系谣言，那么换帅俱乐部会是哪几家呢？  disagreed\n",
       "216510     最火的团队年终奖：王者荣耀团队年终奖：100个月的工资  王者荣耀：农药团队年终奖100个月工资，官方辟谣不属实  disagreed\n",
       "232629            桃子和西瓜一起吃会致命？专家：我常一起吃       「读网」桃子西瓜一起吃不会丧命 不要相信谣言  disagreed\n",
       "...                                ...                          ...        ...\n",
       "174764                        张卫健吸毒被带走     张卫健辟谣什么 事件来龙去脉曝光一个举动被传吸毒  disagreed\n",
       "113003            又一明星倒下？宋小宝被传患癌去世，不会吧    以讹传讹，宋小宝去世了！别逗了，宋小宝都出来辟谣了  disagreed\n",
       "294025         网传“2017年免缴新农合医保费”，你当真了？    辟谣：2017年新农合不用缴费，已开始退款？假的！  disagreed\n",
       "257810                   独生子女将无法继承父母房产             独生子女将无法继承父母房产？谣言  disagreed\n",
       "128097     喝酒没开车也算酒驾！又来一条新交规，昆明老司机们赶紧看     安徽车主速看！酒后躺车内算酒驾吗？交警辟谣了……  disagreed\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.label == 'disagreed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集数据\n",
    "df_test = pd.read_csv('fake-news-pair-classification-challenge/test.csv')\n",
    "df_test = df_test.loc[:, ['title1_zh', 'title2_zh', 'id']]\n",
    "df_test.columns = ['text_a', 'text_b', 'label']\n",
    "df_test.to_csv('test.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT:\n",
    "\n",
    "![BERT sentence pair encoding](https://leemeng.tw/images/bert/practical_bert_encoding_for_pytorch.jpg)\n",
    "\n",
    "tokens_tensor：代表識別每個 token 的索引值，用 tokenizer 轉換即可\n",
    "\n",
    "segments_tensor：用來識別句子界限。第一句為 0，第二句則為 1。另外注意句子間的 [SEP] 為 0\n",
    "\n",
    "masks_tensor：用來界定自注意力機制範圍。1 讓 BERT 關注該位置，0 則代表是 padding 不需關注\n",
    "\n",
    "* [CLS]：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr.\n",
    "* [SEP]：有兩個句子的文本會被串接成一個輸入序列，並在兩句之間插入這個 token 以做區隔\n",
    "* [UNK]：沒出現在 BERT 字典裡頭的字會被這個 token 取代\n",
    "* [PAD]：zero padding 遮罩，將長度不一的輸入序列補齊方便做 batch 運算\n",
    "* [MASK]：未知遮罩，僅在預訓練階段會用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这', '是', '一', '个', '[UNK]', '的', '句', '子']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "text = '这是一个BERT的句子'\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in ['train', 'test']\n",
    "        self.mode = mode\n",
    "        file_name = mode + '.tsv'\n",
    "        # 创建df，大数据需要使用iterator = True\n",
    "        self.df = pd.read_csv(file_name, sep = '\\t').fillna('')\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed':0, 'disagreed':1, 'unrelated':2}\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # __getitem__可以通过trainset[5]的方式访问\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'test':\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            label_id = self.label_map[label] # 将agreed这种转换成1/2/3的格式\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        \n",
    "        # 第一个句子，转化出来的都是单个的汉字\n",
    "        word_pieces = ['[CLS]']\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + ['[SEP]']\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二个句子\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + ['[SEP]']\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        #将整个句子序列转换成index序列——汉字变成数字   \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([0]*len_a + [1]*len_b, dtype = torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = FakeNewsDataset('train', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102861            别说你都知道，华为手机音量键还有这样的操作！\n",
       "41004     一块陈皮能顶10副药，治疗膝盖疼效果惊人，家有膝盖疼的要存！\n",
       "11974          29岁释小龙回归，前女友嫌弃他，今事业辉煌前任离婚\n",
       "5215      2017这几种袁大头市场才叫火爆，最贵的卖了598万我的天！\n",
       "261192    王思聪放弃网红追腾讯千金，两人机场牵手相约吃饭？网友：我反对\n",
       "                       ...              \n",
       "167715                     岑溪市人民医院有人跳楼身亡\n",
       "173044    建个猪场要十几亿，王健林怒骂：我的钱也不是自己印出来的（4）\n",
       "290002    红包撤回速度能比得上抢红包的速度吗？微信准备加入红包撤回功能\n",
       "201596                 收藏：28股，目标涨幅超过50%！\n",
       "201120             支付宝提现开收手续费 免费攻略备受网友力捧\n",
       "Name: text_a, Length: 2657, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[:,'text_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Origin text_a: \n",
      "别说你都知道，华为手机音量键还有这样的操作！\n",
      "Origin text_b: \n",
      "华为手机音量键还隐藏了这么多功能，玩手机这么久才发现，快试试\n",
      "\n",
      "Text tensors: \n",
      "tensor([ 101, 1166, 6432,  872, 6963, 4761, 6887, 8024, 1290,  711, 2797, 3322,\n",
      "        7509, 7030, 7241, 6820, 3300, 6821, 3416, 4638, 3082,  868, 8013,  102,\n",
      "        1290,  711, 2797, 3322, 7509, 7030, 7241, 6820, 7391, 5966,  749, 6821,\n",
      "         720, 1914, 1216, 5543, 8024, 4381, 2797, 3322, 6821,  720,  719, 2798,\n",
      "        1355, 4385, 8024, 2571, 6407, 6407,  102])\n",
      "\n",
      "Segments_tensor: \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "Label: 0\n",
      "\n",
      "Convert text: \n",
      "[CLS]别说你都知道，华为手机音量键还有这样的操作！[SEP]华为手机音量键还隐藏了这么多功能，玩手机这么久才发现，快试试[SEP]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "text_a, text_b, label = trainset.df.iloc[index].values\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[index]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = ''.join(tokens)\n",
    "\n",
    "print(f'''\n",
    "\n",
    "Origin text_a: \\n{text_a}\n",
    "Origin text_b: \\n{text_b}\n",
    "\n",
    "Text tensors: \\n{tokens_tensor}\n",
    "\n",
    "Segments_tensor: \\n{segments_tensor}\n",
    "\n",
    "Label: {label_tensor}\n",
    "\n",
    "Convert text: \\n{combined_text}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 1166, 6432,  872, 6963, 4761, 6887, 8024, 1290,  711, 2797, 3322,\n",
       "         7509, 7030, 7241, 6820, 3300, 6821, 3416, 4638, 3082,  868, 8013,  102,\n",
       "         1290,  711, 2797, 3322, 7509, 7030, 7241, 6820, 7391, 5966,  749, 6821,\n",
       "          720, 1914, 1216, 5543, 8024, 4381, 2797, 3322, 6821,  720,  719, 2798,\n",
       "         1355, 4385, 8024, 2571, 6407, 6407,  102]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 区分了有无labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "        \n",
    "    # 通过pad_sequence 变成同样的长度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first = True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first = True)\n",
    "    \n",
    "    # attention mask\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype = torch.long)\n",
    "    #Fills elements of self tensor with value where mask is True. \n",
    "    masks_tensors = masks_tensors.masked_fill(mask = tokens_tensors != 0,\n",
    "                                            value = 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE,\n",
    "                        collate_fn = create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 63]) \n",
      "tensor([[ 101, 2798, 4761,  ..., 4761, 6887,  102],\n",
      "        [ 101, 3448, 3719,  ...,    0,    0,    0],\n",
      "        [ 101, 1266,  776,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  126, 3299,  ...,    0,    0,    0],\n",
      "        [ 101, 1283,  674,  ...,    0,    0,    0],\n",
      "        [ 101, 2750, 1052,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([64, 63])\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 63])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2,\n",
      "        2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 1, 2, 2, 2, 0, 0,\n",
      "        0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e45befec9a444a819a615098f58717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=568.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a89cf3d3a44e6b20acf1b7cac5ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411577189.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 在上游的Bert模型之下构建Linear classifier\n",
    "# 记得在pycharm上看一眼model\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels = NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb                          test.tsv\r\n",
      "\u001b[34mfake-news-pair-classification-challenge\u001b[m\u001b[m train.tsv\r\n",
      "pytorch_train.tsv\r\n"
     ]
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc = False):\n",
    "    # 初始化一些参数\n",
    "    # 在no-grad下运算\n",
    "    # output就是把相关三个参数输入到model当中去\n",
    "    # 然后通过max取出预测值，然后跟实际y值比较\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predictions = None\n",
    "        \n",
    "        for data in dataloader:\n",
    "            \n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to('cuda:0') for t in data if t is not None]\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids = tokens_tensors, token_type_ids = segments_tensors, attention_mask = masks_tensors)\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            \n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "            \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6157320286036884"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1403ba5ed67c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "\n",
    "EPOCHS = 6\n",
    "for e in range(EPOCHS):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids = tokens_tensors, token_type_ids = segments_tensors, \n",
    "                        attention_mask = masks_tensors, labels = labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    _, acc = get_predictions(model, trainloader, compute_acc = True)\n",
    "    \n",
    "    print('Epoch: {}\\tRunning_loss:{.3f}\\tAccurancy:{.3f}').format(epoch+1, running_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
